ğŸš• NYC Cab Trip Analysis using PySpark  
This project showcases how to ingest NYC Taxi trip datasets into Azure Data Lake / Blob Storage / Databricks, transform it with PySpark DataFrames, and execute a variety of analytical tasks to gather insights on fares, high-traffic zones, and payment statistics.

ğŸ“ Project Layout  
NYC-Cab-Trip-Analysis/  
â”‚  
â”œâ”€â”€ dataset/              # Raw data samples (CSV or JSON)  
â”œâ”€â”€ notebooks/            # PySpark exploration notebooks  
â”œâ”€â”€ results/              # Output files in Parquet / query results  
â”œâ”€â”€ etl_jobs/             # PySpark ETL transformation scripts  
â”œâ”€â”€ README.md             # Project summary and steps  

ğŸ“¥ Data Reference  
The data used in this demo originates from NYC TLC's official trip records:

Trip Data Sample (Yellow Cabs - February 2020)  
ğŸ”— Download CSV  
ğŸ“‚ Official Source: NYC Taxi and Limousine Commission  

ğŸ”§ Workflow Summary  

ğŸ“Œ Load Trip Data  
- Ingest CSV records into DBFS (Databricks File System)  

ğŸ“Œ Data Cleanup & Shaping  
- Process data using PySpark DataFrame operations  
- Flatten any nested JSON if applicable  

ğŸ“Œ Persist Processed Data  
- Store transformed records as external Parquet tables for query usage  

ğŸ“Š Key Queries Executed  
All queries below utilize either Spark SQL or DataFrame transformations within Databricks notebooks.

âœ… Query 1: Compute Earnings Column  
Append a new column `Earnings` by summing:  
- fare_amount  
- extra  
- mta_tax  
- improvement_surcharge  
- tip_amount  
- tolls_amount  
- total_amount  

âœ… Query 2: Total Passengers by Zone  
Aggregate total number of passengers grouped by pickup zone  

âœ… Query 3: Vendor Revenue Stats (Live)  
Calculate live average fare and cumulative revenue by vendor ID  

âœ… Query 4: Analyze Payment Preferences  
Use window function or streaming to count occurrences of each payment method over time  

âœ… Query 5: Top Vendors for a Given Day  
Identify the top 2 vendors by total earnings on a specific date, including passenger count and total miles  

âœ… Query 6: Highest Traffic Route  
Determine the route (pickup â†’ dropoff pair) with the most passengers  

âœ… Query 7: Live Popular Pickup Areas  
Track top pickup locations by passenger volume in the last few seconds using streaming window logic  

ğŸ’» Stack & Tools  
- Databricks / Azure Blob Storage / Data Lake  
- Apache Spark (via PySpark)  
- Parquet Storage Format  
- Spark SQL / DataFrame API  
- Streaming / Window-based processing (optional)  

ğŸ§  Core Concepts Learned  
- Performing large-scale data cleanup and summarization with PySpark  
- Applying real-time and batch techniques using Spark APIs  
- Designing efficient Spark queries using DataFrame and SQL mix  
- Managing external storage using columnar formats like Parquet  

ğŸ“ Helpful Snippets  

# Read raw trip data from mounted blob storage  
df = spark.read.csv("/mnt/storage/yellow_tripdata_2020-02.csv", header=True, inferSchema=True)  

# Save cleaned data as Parquet for analytics  
df.write.mode("overwrite").parquet("/mnt/results/yellow_tripdata_cleaned.parquet")  

